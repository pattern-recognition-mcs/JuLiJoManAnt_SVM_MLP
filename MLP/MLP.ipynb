{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of pytorch\n",
    "\n",
    "* Easy to use\n",
    "* Dynamic gestion of the computational graph\n",
    "* Well used in research\n",
    "* Good documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from train_tools import TrainHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> mnist_train.csv loaded\n",
      "> mnist_test.csv loaded\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "TRAIN_DATASET = \"./../data/csv/mnist_train.csv\"\n",
    "TEST_DATASET = \"./../data/csv/mnist_test.csv\"\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"Load the data of the file given in parameter.\"\"\"\n",
    "    \n",
    "    data = np.loadtxt(filename, delimiter=\",\")\n",
    "    train = np.asfarray(data[:, 1:])\n",
    "    labels = np.reshape(data[:, :1], -1)\n",
    "\n",
    "    print(\"> {} loaded\".format(filename.split(\"/\")[-1]))\n",
    "    \n",
    "    return train, labels\n",
    "\n",
    "# Load the training and the test set.\n",
    "training_data, training_labels = load_data(TRAIN_DATASET)\n",
    "test_data, test_labels = load_data(TEST_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training set in 2. The training set 85% and the validation set 15%.\n",
    "size_train = int(training_data.shape[0]*0.85)\n",
    "size_val = training_data.shape[0]-size_train\n",
    "\n",
    "indices = np.random.permutation(training_data.shape[0])\n",
    "train_idx, val_idx = indices[:size_train], indices[size_train:]\n",
    "\n",
    "train_data, val_data = training_data[train_idx,:], training_data[val_idx,:]\n",
    "train_labels, val_labels = training_labels[train_idx], training_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MnistDataset\n",
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        datapoint = self.data[index]\n",
    "        target = self.target[index]\n",
    "        return torch.tensor(datapoint, dtype=torch.float),\\\n",
    "               torch.tensor(target, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_dataset = MnistDataset(train_data, train_labels)\n",
    "val_dataset = MnistDataset(val_data, val_labels)\n",
    "test_dataset = MnistDataset(test_data, test_labels)\n",
    "\n",
    "# Create data loader\n",
    "params = {'batch_size': 32,\n",
    "          'shuffle': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, **params)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, **params)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(nb_epochs, train, val, step):\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(nb_epochs), train)\n",
    "    plt.plot(np.arange(nb_epochs), val)\n",
    "    plt.legend(['training', 'validation'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(f'{step} value')\n",
    "    plt.title(f'Train/val {step}');\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        #print(input_.shape)\n",
    "        # input_ = input_.view(input_.size(0), -1)\n",
    "        #print(input_.shape)\n",
    "        return self.layers(input_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dims = [i for i in range(10, 101, 10)]\n",
    "output_dim = 10\n",
    "l_rates = [1*10**-i for i in range(1, 4)]\n",
    "nb_epochs = 15\n",
    "trainer = TrainHelper(nb_epochs)\n",
    "\n",
    "# best parameters: (validation acc, hidden dim, learning rate)\n",
    "best_parameters = (float(\"-INF\"), None, None, None)\n",
    "# keep a copy of the best trained network\n",
    "best_model = None\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for hidden_dim, l_rate in product(hidden_dims, l_rates):\n",
    "    neural_net = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "    optimizer = torch.optim.SGD(neural_net.parameters(), l_rate)\n",
    "    \n",
    "    \n",
    "    print(f'Hidden dim: {hidden_dim}, learning rate: {l_rate}')\n",
    "    stats_training = trainer.fit(neural_net,\n",
    "                                 train_loader,\n",
    "                                 val_loader,\n",
    "                                 optimizer,\n",
    "                                 loss_function)\n",
    "    \n",
    "    if stats_training[3][-1] > best_parameters[0]:\n",
    "        best_parameters = (stats_training[3][-1], hidden_dim, l_rate, stats_training)\n",
    "        best_model = deepcopy(neural_net)\n",
    "\n",
    "best_acc, best_hidden_dim, best_l_rate, best_stats = best_parameters\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = best_stats\n",
    "\n",
    "print(f'\\n\\nBest parameters: {best_hidden_dim} hidden dim, \\\n",
    "{best_l_rate} learning_rate and validation accuracies: {best_acc:.2f}%')\n",
    "\n",
    "plot_graph(nb_epochs, train_losses, val_losses, \"Loss\")\n",
    "plot_graph(nb_epochs, train_accuracies, val_accuracies, \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, acc_test = trainer.validation(best_model, test_loader, loss_function)\n",
    "print(f'Accuracy on the test dataset {acc_test:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
